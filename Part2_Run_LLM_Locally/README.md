# Part 2: Run LLM Locally

In this part of the tutorial, you'll learn how to switch from a cloud-based LLM like OpenAI to a locally running model using [Ollama](https://ollama.com/). This allows for offline development, cost savings, and enhanced data privacy.

## Goal

The goal is to modify the Restaurant Menu Agent from Part 1 to use a local LLM for chat completions, running everything on your own machine.

## What You'll Learn

- How to install and run a local LLM with Ollama.
- How to integrate the Semantic Kernel with an Ollama-served model.
- The benefits and trade-offs of using a local LLM versus a cloud-based service.

## How to Run This Project

### 1. Install and Set Up Ollama

First, you need to install Ollama on your system.

1.  **Download and install Ollama:**
    - Run Ollam on docker

    docker run --gpus all -d -v ollama_data:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:latest

2.  **Pull a model:**
    - After installing Ollama, you need to download a model. We recommend starting with `llama3`, as it provides a good balance of performance and resource requirements. 
    - docker exec -it ollama ollama pull llama3
    - docker exec -it ollama ollama pull mistral
    - docker exec -it ollama ollama pull deepseek-r1

3.  **Ensure Ollama is running:**
    - Check docker hub to see if Ollama is running 

4. **Ensure LLM is running:**
    - docker exec -it ollama ollama ps -- shows the running LLM model
    - docker exec -it ollama ollama list -- shows all available LLMs
    - docker exec -it ollama ollama run <llm_name> -- runs a specific LLM llm_name could be llam3, mistral etc.


### 2. Configure the Model (Optional)

By default, the project is configured to use the `llama3:latest` model. If you want to use a different model, you can configure it using the .NET User Secrets store.

1.  **Initialize User Secrets (if you haven't already):**
    Open a terminal in this directory (`Part2_Run_LLM_Locally`) and run:
    ```bash
    dotnet user-secrets init
    ```

2.  **Set the ModelId:**
    Run the following command, replacing `"your_model_id"` with the ID of the model you want to use (e.g., `mistral:latest`):
    ```bash
    dotnet user-secrets set "Ollama:ModelId" "your_model_id"
    ```

### 3. Run the Application

Once Ollama is set up and running, you can run the .NET project.

**Using the .NET CLI:**

1.  Open a terminal in this directory (`Part2_Run_LLM_Locally`).
2.  Run the application:
    ```bash
    dotnet run
    ```

**Using Visual Studio or Rider:**

- Open the `Part2_Run_LLM_Locally.sln` file.
- Set `Part2_Run_LLM_Locally` as the startup project.
- Press the "Run" button.

### 4. Interact with the Agent

The console application will start, and you can now chat with the agent, which is powered by your local Llama 3 model. The interaction will be similar to Part 1, but the responses will be generated by your local LLM. Example questions:

- "List all vegan dishes on the menu."
- "What are the chicken dishes on menu?"
- "What is the most expensive item on the menu?"

## Project Structure

- **`Program.cs`**: The main entry point, containing the chat loop.
- **`KernelServiceLLM.cs`**: A modified version of the `KernelService` from Part 1. This class is configured to connect to the local Ollama service instead of OpenAI.
- **`MenuPlugin.cs`**: The same plugin as in Part 1, providing the restaurant menu knowledge.
- **`Part2_Run_LLM_Locally.csproj`**: The project file, which includes the `Microsoft.SemanticKernel.Connectors.Ollama` package.

With this, you are now running a fully local AI agent! Next, in [Part 3](./../Part3_McpServer/README.md), you will explore a more advanced client/server architecture. 